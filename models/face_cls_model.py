"""
Author: Redal
Date: 2025/03/22
TODO: 提供三种人脸识别方式: 1.dlib特征提取; 2.facenet特征检测; 
      3.face_recognition高度集成特征检测(Opt_best)
Homepage: https://github.com/Rtwotwo/MMchat.git
"""
import os
import sys
import dlib
import cv2
import torch
import json
import argparse
import numpy as np
from PIL import Image, ImageDraw
import face_recognition as fr
from facenet_pytorch import InceptionResnetV1
from torchvision.transforms import transforms

project_path = os.getcwd()
current_path = os.path.join(os.path.dirname(__file__))
sys.path.append(current_path)
sys.path.append(project_path)
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
os.environ['CUDA_VISIBLE_DEVICES'] = '0'



##########################  变量阈定义处理  ##########################
def face_config():
      parser = argparse.ArgumentParser(description='Face Recognition models and methods',
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
      parser.add_argument_group('Dlib Extractor')
      parser.add_argument('--dlib_dirs', type=str, default=os.path.join(project_path, 'weights'),
                        help='the directory of dlib weights')
      parser.add_argument('--dlib_predictor', type=str, default='face_predictor_68_face_landmarks.dat',
                        help='the name of dlib dlib predictor')
      parser.add_argument('--dlib_resnet', type=str, default='face_recognition_resnet_model_v1.dat',
                        help='the name of dlib resnet model for face recognition')
      
      parser.add_argument_group('FaceNet Extractor')
      parser.add_argument('--facenet_dir', type=str, default='./weights', 
                          help='the directory of facenet model')
      parser.add_argument('--facenet_name', type=str, default='facenet-vggface2.pt',
                          help='the name of facenet model facenet-vggface2.pt / facenet-webface.pt')
      parser.add_argument('--facenet_flag', type=str, default='vggface2',
                          help='the name of facenet model vggface2 / casia-webface')
      args =parser.parse_args()
      return args



##########################  人脸特征提取注册算法  ##########################
class DlibExtractor(object):
      """use dlib to extract face features and descriptors
      :param input: the camera RGB frame, format cv2-ndarray
      :param return: face descriptors, format [128, 1]
      :param args: generated by config function """
      def __init__(self, input_img, args, **kwargs):
            self.args = args
            predictor_path = os.path.join(self.args.dlib_dirs, self.args.dlib_predictor)
            resnet_path = os.path.join(self.args.dlib_dirs, self.args.dlib_resnet)
            # Initialize model functions
            self.predictor = dlib.shape_predictor(predictor_path)
            self.face_rec = dlib.face_recognition_model_v1(resnet_path)
            self.detector = dlib.get_frontal_face_detector()
            # process input frame, usually is gray image
            self.input = np.array(input_img)
      def __extract__(self):
            """extract face descriptors for recognition"""
            faces = self.detector(self.input)
            try: shape = self.predictor(self.input, faces[0])
            except: RuntimeError('No Face has been detected!')
            face_descriptor = self.face_rec.compute_face_descriptor(self.input, shape)
            return face_descriptor


class FaceNetExtractor(object):
      """use facenet to extract face features and descriptors
      :param input: the camera RGB frame,format cv2-ndarray
      :param return: the face descriptors format [1, 512]
      :param args: generated by config function """
      def __init__(self, input_img, args, **kwargs):
            self.args = args
            self.input = Image.fromarray(input_img).convert('RGB')
            self.weight_path = os.path.join(args.facenet_dir, args.facenet_name)
            # load facenet model's weights
            self.model = self.__download__()
            self.facenet_transform = transforms.Compose([
                  transforms.ToTensor(),
                  transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])
      def __download__(self):
            """download facenet model
            :returen :the loaded weights model"""
            if self.args.facenet_flag == 'vggface2':
                  infer_model = InceptionResnetV1(pretrained='vggface2').eval()
            elif self.args.facenet_flag == 'casia-webface':
                  infer_model = InceptionResnetV1(pretrained='casia-webface').eval()
            else: RuntimeError('====face model already exists!====')
            return infer_model
      def __extract__(self):
            """extract face descriptors for recognition"""
            self.input = self.facenet_transform(self.input).unsqueeze(0)
            with torch.no_grad(): self.embedding = self.model(self.input)
            return self.embedding.detach().cpu().numpy()


class FaceRecognition(object):
      """use face-recognition model to recognize face and
      recognise all the faces in the frame, format [1, 128]
      :param input: the camera RGB frame, format cv2-ndarray
      :param return: the face descriptors format list
      :param args: generated by config function """
      def __init__(self, args, **kwargs):
            self.args = args
      def __extract__(self, input_img, all_faces=False):
            # extract all faces encodings 
            self.input = np.array(input_img)
            face_locations = fr.face_locations(self.input)
            if all_faces: 
                  face_encodings = fr.face_encodings(self.input, face_locations)
                  return face_locations, face_encodings
            else: 
                  face_encodings = fr.face_encodings(self.input, face_locations[0:1])
                  return face_locations[0:1] ,face_encodings
      def __compare__(self, input_img, json_fpath):
            # compare the face encodings with the known face encodings
            with open(json_fpath, 'r')  as f:
                  known_dict = f.load()
                  known_encodings = known_dict['encodings']
                  known_names = known_dict['names']
            pil_img = Image.fromarray(input_img)
            draw_img = ImageDraw.Draw(pil_img)
            # compute current frame faces encodings
            face_locations, face_encodings = self.__extract__(input_img, all_faces=True)
            for [up, left, bottom, right], face_encoding in zip(face_locations, face_encodings):
                  matches = fr.compare_faces(known_encodings, face_encoding)
                  if True in matches:
                        name = known_names[matches.index(True)]
                  else: name = 'Unknown'
                  # plot known face name and bounding box
                  draw_img.rectangle([left, up, right, bottom], outline=(0, 0, 255))
                  draw_img.text((left, up - 10), name, fill=(0, 0, 255))
            return np.array(pil_img)



##########################  主函数测试分析  ##########################
if __name__=='__main__':
      """Testing the facial recognition model"""
      args = face_config()
      input = cv2.imread(r'assets/face_cls/multi_face.jpg',cv2.IMREAD_COLOR_RGB)

      # 1. Dlib Extractor
      dlib_extractor = DlibExtractor(input, args)
      print(dlib_extractor.__extract__().shape)

      # 2. FaceNet Extractor
      facenet_extractor = FaceNetExtractor(input, args)
      print(facenet_extractor.__extract__().shape)

      # 3. FaceRecognition Extractor
      faceRe = FaceRecognition(input, args)
      _, face_encoding = faceRe.__extract__(input, all_faces=False)
      print(np.array(face_encoding).shape)